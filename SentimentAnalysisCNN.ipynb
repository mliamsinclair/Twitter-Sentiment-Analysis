{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HArpK-pcGC7Y",
        "outputId": "426eec3c-86d2-441c-d23c-99aa6229e8b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.10/dist-packages (0.0.11)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.9.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "# install required libraries\n",
        "! pip install tensorflow scikit-learn pandas numpy pickle5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P2AGmbrGQk3"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "from tensorflow import keras\n",
        "from google.colab import files, drive  # Importing libraries for file handling and drive mounting\n",
        "import os  # importing the os module for operating system related functionalities\n",
        "import numpy as np  # importing numpy for numerical computations\n",
        "import pandas as pd  # importing pandas for data manipulation and analysis\n",
        "import tensorflow as tf  # importing tensorflow for deep learning\n",
        "from sklearn.model_selection import KFold # importing kfold for cross-validation\n",
        "from sklearn.metrics import accuracy_score # importing accuracy score for cross-validation\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer  # importing Tokenizer for text preprocessing\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences  # importing pad_sequences for sequence padding\n",
        "from tensorflow.keras.models import Sequential  # importing Sequential model for building deep learning models\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, BatchNormalization  # importing different layers for building the model\n",
        "from tensorflow.keras.utils import to_categorical  # importing to_categorical for one-hot encoding\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau  # importing callbacks for model training\n",
        "from tensorflow.keras.regularizers import l2  # importing l2 regularization for model regularization\n",
        "from sklearn.model_selection import train_test_split  # importing train_test_split for splitting the data\n",
        "from sklearn.metrics import classification_report, confusion_matrix  # importing metrics for model evaluation\n",
        "import pickle  # importing pickle for object serialization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mounting Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# setting the base directory for saving model and tokenizer\n",
        "base_dir = '/content/drive/My Drive/Colab Notebooks/MyModel/'\n",
        "\n",
        "# setting the paths for saving the model/tokenizer\n",
        "model_path = os.path.join(base_dir, 'twitter_sa_model.keras')\n",
        "tokenizer_path = os.path.join(base_dir, 'tokenizer.pickle')\n",
        "\n",
        "# creating the base directory if it doesn't exist\n",
        "os.makedirs(base_dir, exist_ok=True)\n",
        "\n",
        "# setting the path for the dataset\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/MyModel/cleaned_dataset.h5'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntKo17Qzx-Cf",
        "outputId": "49047396-eccb-4ffd-a1d0-f367802c3b05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YraxzFYLMs_"
      },
      "outputs": [],
      "source": [
        "# reading the dataset into a pandas DataFrame\n",
        "df = pd.read_hdf(file_path, key='df')\n",
        "\n",
        "# extracting the features from the DataFrame\n",
        "features = df['cleaned_text']\n",
        "\n",
        "# extracting the target labels from the DataFrame\n",
        "target_labels = df['target']\n",
        "\n",
        "# initializing the tokenizer with a maximum vocabulary size of 20000\n",
        "tokenizer = Tokenizer(num_words=20000, oov_token='<OOV>')\n",
        "\n",
        "# fitting the tokenizer on the features\n",
        "tokenizer.fit_on_texts(features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing function\n",
        "def preprocess_data(features, tokenizer, max_len=100):\n",
        "    # convert features into sequences\n",
        "    sequences = tokenizer.texts_to_sequences(features)\n",
        "    # pad the sequences to a fixed length of 100\n",
        "    padded_sequences = pad_sequences(sequences, maxlen=max_len, truncating='post')\n",
        "    return padded_sequences"
      ],
      "metadata": {
        "id": "nxiB4S6i0uVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the data\n",
        "padded_sequences = preprocess_data(features, tokenizer)\n",
        "\n",
        "# one-hot encoding the target labels\n",
        "labels = to_categorical(target_labels, num_classes=2)\n",
        "\n",
        "# splitting the data into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "BPe49BAg1ANJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing an empty dictionary for word embeddings\n",
        "embedding_index = {}\n",
        "\n",
        "# opening the file containing word embeddings\n",
        "with open('/content/drive/MyDrive/SeniorPortfolio/glove.twitter.27B.200d.txt', 'r', encoding='utf-8') as f:\n",
        "    # iterating over each line in the file\n",
        "    for line in f:\n",
        "        # splitting the line into values\n",
        "        values = line.split()\n",
        "        # extracting the word\n",
        "        word = values[0]\n",
        "        # converting the values into a numpy array\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        # storing the word embedding in the dictionary\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "# setting the dimensionality of the word embeddings\n",
        "embedding_dim = 200\n",
        "\n",
        "# setting the maximum number of words in the vocabulary\n",
        "max_words = 20000\n",
        "\n",
        "# initializing the embedding matrix with zeros\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))"
      ],
      "metadata": {
        "id": "2fqD4Cf1zdsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# iterating over each word in the tokenizer's word index\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    # checking if the word index is within the maximum number of words\n",
        "    if i < max_words:\n",
        "        # getting the word embedding vector\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        # checking if the word has an embedding vector\n",
        "        if embedding_vector is not None:\n",
        "            # storing the word embedding vector in the embedding matrix\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "g-eiQ0Qszduc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a sequential model\n",
        "model = Sequential([\n",
        "    # adding an embedding layer with pre-trained word embeddings\n",
        "    Embedding(max_words, embedding_dim, input_length=100, weights=[embedding_matrix], trainable=True),\n",
        "    # adding a 1D convolutional layer with 64 filters and a kernel size of 5\n",
        "    Conv1D(64, 5, activation='relu', padding='same'),\n",
        "    # adding batch normalization\n",
        "    BatchNormalization(),\n",
        "    # adding another 1D convolutional layer with 128 filters and a kernel size of 5\n",
        "    Conv1D(64, 5, activation='relu', padding='same'),\n",
        "    # adding batch normalization\n",
        "    BatchNormalization(),\n",
        "    # adding global max pooling\n",
        "    GlobalMaxPooling1D(),\n",
        "    # adding a dense layer with 64 units and ReLU activation\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.01)),\n",
        "    # adding dropout regularization\n",
        "    Dropout(0.6),\n",
        "    # adding a dense layer with 2 units and softmax activation\n",
        "    Dense(2, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "VIL2m9ARzdwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compiling the model with Adam optimizer, categorical crossentropy loss, and accuracy metric\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# printing the summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "SsLG8wKZzdyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initializing early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)\n",
        "\n",
        "# initializing model checkpoint callback\n",
        "model_checkpoint = ModelCheckpoint(filepath=os.path.join(base_dir,'best_sentiment_analysis_model.keras'), save_best_only=True, monitor='val_accuracy', verbose=1)\n",
        "\n",
        "# initializing reduce learning rate callback\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-5)"
      ],
      "metadata": {
        "id": "a99STuvVzd07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, BatchNormalization, GlobalMaxPooling1D, Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# *** NOTE ***\n",
        "# this current build focuses on hyperparameter tuning for the\n",
        "# final model's training\n",
        "# define search space\n",
        "space = {\n",
        "    'dropout_rate': hp.uniform('dropout_rate', 0.1, 0.7),\n",
        "    'l2_regularization': hp.uniform('l2_regularization', 0.0001, 0.01),\n",
        "    'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.01)),\n",
        "}\n",
        "\n",
        "def objective(space):\n",
        "    print(f\"Evaluating with hyperparameters: {space}\")\n",
        "    model = Sequential([\n",
        "        Embedding(max_words, embedding_dim, input_length=100, weights=[embedding_matrix], trainable=True),\n",
        "        Conv1D(64, 5, activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        Conv1D(64, 5, activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        GlobalMaxPooling1D(),\n",
        "        Dense(64, activation='relu', kernel_regularizer=l2(space['l2_regularization'])),\n",
        "        Dropout(space['dropout_rate']),\n",
        "        Dense(2, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=Adam(learning_rate=space['learning_rate']),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # splitting data into training and validation set\n",
        "    x_train, x_val, y_train, y_val = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # creating a smaller subset for hyperparameter tuning\n",
        "    subset_size = int(0.2 * len(x_train))  # Using 20% of the training data\n",
        "    x_train_subset = x_train[:subset_size]\n",
        "    y_train_subset = y_train[:subset_size]\n",
        "\n",
        "\n",
        "    # fit the model\n",
        "    history = model.fit(x_train_subset, y_train_subset, validation_data=(x_val, y_val), epochs=10, batch_size=32, verbose=2, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "    # calculate the minimum validation loss\n",
        "    min_val_loss = min(history.history['val_loss'])\n",
        "\n",
        "    # print max accuracy\n",
        "    max_acc = max(history.history['val_accuracy'])\n",
        "    print(\"Max accuracy: \", max_acc)\n",
        "\n",
        "    return {'loss': min_val_loss, 'status': STATUS_OK}\n",
        "\n",
        "# trials object to store each experiment's results\n",
        "trials = Trials()\n",
        "\n",
        "# run the optimization\n",
        "best_hyperparams = fmin(fn=objective,\n",
        "                        space=space,\n",
        "                        algo=tpe.suggest,\n",
        "                        max_evals=20,\n",
        "                        trials=trials)\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_hyperparams)\n"
      ],
      "metadata": {
        "id": "aqDSZXrG9I19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating the model on the test set\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "\n",
        "# printing the test accuracy\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "\n",
        "# predicting the labels for the test set\n",
        "y_pred = np.argmax(model.predict(x_test), axis=1)\n",
        "\n",
        "# getting the true labels for the test set\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# printing the classification report\n",
        "print(classification_report(y_true, y_pred, target_names=['Class1', 'Class2']))\n",
        "\n",
        "# printing the confusion matrix\n",
        "print(confusion_matrix(y_true, y_pred))"
      ],
      "metadata": {
        "id": "SrzKn5Ijzd3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the model\n",
        "model.save(model_path)\n",
        "\n",
        "# saving the tokenizer\n",
        "with open(tokenizer_path, 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "zfz6pIgIzd5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the saved model\n",
        "model = keras.models.load_model(model_path)\n",
        "\n",
        "# loading the saved tokenizer\n",
        "with open(tokenizer_path, 'rb') as handle:\n",
        "    tokenizer = pickle.load(handle)"
      ],
      "metadata": {
        "id": "MTigb474z12O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a function to predict sentiment\n",
        "def predict_sentiment(text):\n",
        "    # converting the text into a sequence\n",
        "    text_sequence = tokenizer.texts_to_sequences([text])\n",
        "    # padding the sequence\n",
        "    text_sequence = pad_sequences(text_sequence, maxlen=100)\n",
        "    # predicting the sentiment\n",
        "    predicted_rating = model.predict(text_sequence)[0]\n",
        "    # getting the index of the predicted sentiment\n",
        "    predicted_index = np.argmax(predicted_rating)\n",
        "    # mapping the index to sentiment label\n",
        "    sentiment = 'Positive' if predicted_index == 1 else 'Negative'\n",
        "    # getting the probability of the predicted sentiment\n",
        "    probability = predicted_rating[predicted_index]\n",
        "    # returning the probability and sentiment\n",
        "    return f\"{probability:.2f} {sentiment}\""
      ],
      "metadata": {
        "id": "31ZhPPflz14k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a softmax function with temperature\n",
        "def softmax(x, temperature=1.0):\n",
        "    # computing the exponential of the input with temperature adjustment\n",
        "    e_x = np.exp((x - np.max(x)) / temperature)\n",
        "    # returning the softmax probabilities\n",
        "    return e_x / e_x.sum(axis=-1)"
      ],
      "metadata": {
        "id": "w8joECy7z165"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a function to predict sentiment with temperature adjustment\n",
        "def predict_sentiment_with_temperature(text, temperature=1.0):\n",
        "    # converting the text into a sequence\n",
        "    text_sequence = tokenizer.texts_to_sequences([text])\n",
        "    # padding the sequence\n",
        "    text_sequence = pad_sequences(text_sequence, maxlen=100)\n",
        "    # predicting the sentiment\n",
        "    predicted_rating = model.predict(text_sequence)[0]\n",
        "    # applying softmax with temperature adjustment\n",
        "    predicted_rating = softmax(predicted_rating, temperature=temperature)\n",
        "    # getting the index of the predicted sentiment\n",
        "    predicted_index = np.argmax(predicted_rating)\n",
        "    # mapping the index to sentiment label\n",
        "    sentiment = 'Positive' if predicted_index == 1 else 'Negative'\n",
        "    # getting the probability of the predicted sentiment\n",
        "    probability = predicted_rating[predicted_index]\n",
        "    # returning the probability and sentiment\n",
        "    return f\"{probability:.2f} {sentiment}\""
      ],
      "metadata": {
        "id": "9SddWHKL0A5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the input text\n",
        "text_input = \"hahahahaha\"\n",
        "\n",
        "# predicting the sentiment of the input text\n",
        "predicted_sentiment = predict_sentiment(text_input)\n",
        "\n",
        "# predicting the sentiment of the input text with temperature adjustment\n",
        "predicted_sentiment_with_temp = predict_sentiment_with_temperature(text_input)\n",
        "\n",
        "# printing the predicted sentiment\n",
        "print(predicted_sentiment)\n",
        "\n",
        "# printing the predicted sentiment with temperature adjustment\n",
        "print(predicted_sentiment_with_temp)"
      ],
      "metadata": {
        "id": "nugwZQEP0A7i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}